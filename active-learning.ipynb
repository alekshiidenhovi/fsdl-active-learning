{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning with small-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Features, Value, ClassLabel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from small_text.active_learner import PoolBasedActiveLearner\n",
    "from small_text.base import LABEL_UNLABELED\n",
    "from small_text.integrations.transformers import TransformersDataset, TransformerModelArguments\n",
    "from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n",
    "from small_text.query_strategies import BreakingTies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Found cached dataset ag_news (/Users/alekshiidenhovi/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n",
      "100%|██████████| 2/2 [00:00<00:00, 259.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Configs\n",
    "DATASET_NAME = \"bergr7/weakly_supervised_ag_news\"\n",
    "TRANSFORMER_MODEL = \"distilbert-base-uncased\"\n",
    "LABELS = load_dataset('ag_news')[\"train\"].features[\"label\"].names\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bergr7--weakly_supervised_ag_news-6f78f309523478bd\n",
      "Found cached dataset csv (/Users/alekshiidenhovi/.cache/huggingface/datasets/bergr7___csv/bergr7--weakly_supervised_ag_news-6f78f309523478bd/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 3/3 [00:00<00:00, 927.67it/s]\n",
      "Using custom data configuration bergr7--weakly_supervised_ag_news-9442e7dc9bdd01c3\n",
      "Found cached dataset csv (/Users/alekshiidenhovi/.cache/huggingface/datasets/bergr7___csv/bergr7--weakly_supervised_ag_news-9442e7dc9bdd01c3/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|██████████| 1/1 [00:00<00:00, 536.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# files\n",
    "labeled_data_files = {\n",
    "    \"train\": \"train.csv\",\n",
    "    \"validation\": \"validation.csv\", \n",
    "    \"test\": \"test.csv\"\n",
    "}\n",
    "unlabeled_data_files = {\"unlabeled\": \"unlabeled_train.csv\"}\n",
    "# features\n",
    "labeled_features = Features(\n",
    "    {\n",
    "        \"text\": Value(\"string\"),\n",
    "        \"label\": ClassLabel(\n",
    "            num_classes=4,\n",
    "            names=['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "        )\n",
    "    }\n",
    ")\n",
    "unlabeled_features = Features({\"text\": Value(\"string\")})\n",
    "\n",
    "# load data\n",
    "labeled_dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    data_files=labeled_data_files,\n",
    "    features=labeled_features\n",
    ")\n",
    "\n",
    "unlabeled_dataset = load_dataset(\n",
    "    DATASET_NAME,\n",
    "    data_files=unlabeled_data_files,\n",
    "    features=unlabeled_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:07<00:00,  7.88ba/s]\n",
      "100%|██████████| 38/38 [00:04<00:00,  7.87ba/s]\n",
      "100%|██████████| 24/24 [00:03<00:00,  7.91ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00,  8.50ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
    "\n",
    "# Helper function to tokenize the input text\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize dataset\n",
    "unlabeled_tokenized = unlabeled_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "labeled_tokenized = labeled_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Set convenient output format\n",
    "unlabeled_tokenized.set_format(\"torch\")\n",
    "labeled_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fsdl-active-learning/lib/python3.10/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "train_text = [row[\"text\"] for row in labeled_dataset[\"train\"]]\n",
    "training_labels = np.array([row[\"label\"] for row in labeled_dataset[\"train\"]])\n",
    "target_labels = set(LABELS)\n",
    "\n",
    "# Create the dataset for small-text\n",
    "training_dataset = TransformersDataset.from_arrays(train_text, LABELS, tokenizer, target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_text = [row[\"text\"] for row in labeled_dataset[\"validation\"]]\n",
    "validation_labels = np.array([row[\"label\"] for row in labeled_dataset[\"validation\"]])\n",
    "validation_dataset = TransformersDataset.from_arrays(validation_text, validation_labels, tokenizer, target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = [row[\"text\"] for row in labeled_dataset[\"test\"]]\n",
    "test_dataset = TransformersDataset.from_arrays(test_text, LABELS, tokenizer, target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlabeled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_text = [  \n",
    "  row[\"text\"] for row in unlabeled_dataset[\"unlabeled\"]\n",
    "]\n",
    "# unlabeled_dataset = TransformersDataset.from_arrays(unlabeled_text, LABELS, tokenizer, target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our classifier\n",
    "clf_factory = TransformerBasedClassificationFactory(\n",
    "    TransformerModelArguments(TRANSFORMER_MODEL),\n",
    "    num_classes=len(target_labels),\n",
    "    # If you have a cuda device, specify it here.\n",
    "    # Otherwise, just remove the following line.\n",
    "    kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "# Define our query strategy\n",
    "query_strategy = BreakingTies()\n",
    "\n",
    "# Use the active learner with a pool containing all unlabeled data\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from small_text.initialization import random_initialization\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Number of samples in our queried batches\n",
    "NUM_SAMPLES = 5\n",
    "\n",
    "# Randomly draw an initial subset from the data pool\n",
    "initial_indices = random_initialization(dataset, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rubrix as rb\n",
    "\n",
    "rb.init(api_url=\"http://rubrix:80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the dataset\n",
    "DATASET_NAME = \"test_with_active_learning_test\"\n",
    "\n",
    "# Define labeling schema\n",
    "settings = rb.TextClassificationSettings(label_schema=LABELS)\n",
    "\n",
    "# Create dataset with a label schema\n",
    "rb.configure_dataset(name=DATASET_NAME, settings=settings)\n",
    "\n",
    "# Create records from the initial batch\n",
    "records = [\n",
    "    rb.TextClassificationRecord(\n",
    "        text=ag_news_data[\"train\"][\"text\"][idx],\n",
    "        metadata={\"batch_id\": 0},\n",
    "        id=idx,\n",
    "    )\n",
    "    for idx in initial_indices\n",
    "]\n",
    "\n",
    "# Log initial records to Rubrix\n",
    "rb.log(records, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rubrix.listeners import listener\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define some helper variables\n",
    "# LABEL2INT = ag_news_data[\"train\"].features[\"label\"].str2int\n",
    "LABEL2INT = dict(zip(LABELS, range(4)))\n",
    "ACCURACIES = []\n",
    "\n",
    "# Set up the active learning loop with the listener decorator\n",
    "@listener(\n",
    "    dataset=DATASET_NAME,\n",
    "    query=\"status:Validated AND metadata.batch_id:{batch_id}\",\n",
    "    condition=lambda search: search.total==NUM_SAMPLES,\n",
    "    execution_interval_in_seconds=3,\n",
    "    batch_id=0\n",
    ")\n",
    "def active_learning_loop(records, ctx):\n",
    "\n",
    "    # 1. Update active learner\n",
    "    print(f\"Updating with batch_id {ctx.query_params['batch_id']} ...\")\n",
    "    print('Please go to rubrix to label the data...')\n",
    "    y = np.array([LABEL2INT[rec.annotation] for rec in records])\n",
    "    \n",
    "    print(f\"{NUM_SAMPLES} records have been labeled updating active learner...\")\n",
    "    # initial update\n",
    "    if ctx.query_params[\"batch_id\"] == 0:\n",
    "        indices = np.array([rec.id for rec in records])\n",
    "        active_learner.initialize_data(indices, y)\n",
    "    # update with the prior queried indices\n",
    "    else:\n",
    "        active_learner.update(y)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "\n",
    "    # 2. Query active learner\n",
    "    print(\"Querying new data points ...\")\n",
    "    queried_indices = active_learner.query(num_samples=NUM_SAMPLES)\n",
    "    ctx.query_params[\"batch_id\"] += 1\n",
    "    new_records = [\n",
    "        rb.TextClassificationRecord(\n",
    "            text=ag_news_data[\"train\"][\"text\"][idx],\n",
    "            metadata={\"batch_id\": ctx.query_params[\"batch_id\"]},\n",
    "            id=idx,\n",
    "        )\n",
    "        for idx in queried_indices\n",
    "    ]\n",
    "\n",
    "    # 3. Log the batch to Rubrix\n",
    "    rb.log(new_records, DATASET_NAME)\n",
    "\n",
    "    # 4. Evaluate current classifier on the test set\n",
    "    print(\"Evaluating current classifier ...\")\n",
    "    accuracy = accuracy_score(\n",
    "        dataset_test.y,\n",
    "        active_learner.classifier.predict(dataset_test),\n",
    "    )\n",
    "    ACCURACIES.append(accuracy)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    print(\"Waiting for annotations ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_loop.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_loop.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('fsdl-active-learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ff0eaa1c57213d2d7cae479a56860f7f31eaa0fab84097bfb918daee7f70e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
